{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ollihansen90/linclassifiers/blob/main/jupyternotebooks/Neuroinformatik_exercise_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfpXMUE_L8zE"
      },
      "source": [
        "# Exercise 2: Perceptron Learning and Maximum Margin Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMbk-Q6vMDkb"
      },
      "source": [
        "**Note**: Please insert the names of all participating students:\n",
        "1.\n",
        "2.\n",
        "3."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKReGbJlL8zH"
      },
      "source": [
        "## Preamble\n",
        "The following code downloads and imports all necessary files and modules into the virtual machine of Colab. Please make sure to execute it before solving this exercise. This mandatory preamble will be found on all exercise sheets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rNRcHClL8zM"
      },
      "source": [
        "import sys, os\n",
        "if 'google.colab' in sys.modules:\n",
        "  if os.getcwd() == '/content':\n",
        "    !git clone 'https://github.com/inb-uni-luebeck/cs4405.git'\n",
        "    os.chdir('cs4405')\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from utils import utils_2 as utils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDwhtXssL8zc"
      },
      "source": [
        "## Exercise 2.1: Perceptron Learning\n",
        "Given $L$ training `samples` $\\boldsymbol{x}_i \\in \\mathbb{R}^{1 \\times N}$ and its class `labels` $s_i \\in \\left\\{ -1 , 1 \\right\\}$, we want to train a single artificial neuron, i.e. make it to automatically learn its `weights` $\\boldsymbol{w} \\in \\mathbb{R}^{1 \\times N}$ and its `threshold` $\\theta \\in \\mathbb{R}$, such that\n",
        "\n",
        "$$\n",
        "\\begin{equation}\n",
        "    \\sigma \\left( \\boldsymbol{x}_{i} \\boldsymbol{w}^{T} - \\theta \\right) = s_{i}, \\; \\forall i=1,\\dots,L\n",
        "\\end{equation}\n",
        "$$\n",
        "\n",
        "holds. The sigmoid function is defined as\n",
        "\n",
        "$$\n",
        "\\begin{equation}\n",
        "    \\sigma(x) =\n",
        "        \\left\\{\n",
        "            \\begin{array}{rl}\n",
        "                1, & \\text{if } x \\geq 0 \\\\\n",
        "                -1, & \\text{else}\n",
        "            \\end{array}\n",
        "        \\right.\n",
        "\\end{equation}\n",
        "$$\n",
        "\n",
        "The `weights` $\\boldsymbol{w}$ represents a normal vector of a linear hyperplane and `threshold` $\\theta$ represents its (by $\\lVert \\boldsymbol{w} \\rVert$ scaled) distance to the origin.\n",
        "\n",
        "To simplify learning, we apply the *threshold trick*, i.e. we extend the `weights` $\\boldsymbol{\\hat{w}} \\in \\mathbb{R}^{1 \\times (N + 1)}$ by an additional component in the *first* dimension that represents the `threshold` $\\theta$. The `samples` are likewise extended by a component with constant value $-1$ in the *first* dimension (see `help(np.column_stack)`). In this way, for an extended `sample` $\\boldsymbol{\\hat{x}}_{i} \\in \\mathbb{R}^{1 \\times (N + 1)}$ the output of the neuron can be written as\n",
        "\n",
        "$$\n",
        "\\begin{equation}\n",
        "  y_{i} = \\sigma \\left( \\boldsymbol{\\hat{x}}_{i} \\boldsymbol{\\hat{w}}^{T} \\right)\n",
        "\\end{equation}.\n",
        "$$\n",
        "\n",
        "During each learning `epoch` the given $L$ training `samples` are presented to the artificial neuron in *random order*. We use the perceptron learning rule to adapt the extended `weights` $\\boldsymbol{\\hat{w}}_{t}$ to $\\boldsymbol{\\hat{w}}_{t+1}$\n",
        "\n",
        "$$\n",
        "\\begin{equation}\n",
        "    \\boldsymbol{\\hat{w}}_{t+1} = \\boldsymbol{\\hat{w}}_{t} + \\varepsilon (s_{i} - y_{i}) \\boldsymbol{\\hat{x}}_{i}\n",
        "\\end{equation}\n",
        "$$\n",
        "\n",
        "where\n",
        "\n",
        "$$\n",
        "\\begin{equation}\n",
        "    y_{i} = \\sigma \\left( \\boldsymbol{\\hat{x}}_{i} \\boldsymbol{\\hat{w}}^{T}_{t} \\right)\n",
        "\\end{equation}.\n",
        "$$\n",
        "\n",
        "Here, $\\varepsilon \\in \\mathbb{R}^{+}$ denotes the `learning_rate` and $\\boldsymbol{\\hat{x}}_{i}$ a randomly selected extended training sample.\n",
        "\n",
        "**Tasks**:\n",
        "* Implement the function `learn_perceptron` in Python and test it on the training set `data_2.npz`. Apply your perceptron implementation several times to the example training set (start with a `learning_rate` $\\varepsilon=0.01$).\n",
        "\n",
        "**Programming Hints**:\n",
        "- In each `epoch` of the perceptron learning process a randomly selected training `sample` (see `help(np.random.permutation)`) with class `label` is classified. Then, the `weights` are modified according to the learning rule.\n",
        "- A single learning epoch might not be sufficient for obtaining a correct classification of all training samples. In this case, further learning epochs should be performed until a correct classification is obtained.\n",
        "- Due to time limitations, the function `animation.animate` has a parameter `max_frames` which animates only the first `max_frames` steps of the learning process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wet6j9EgL8zf"
      },
      "source": [
        "def learn_perceptron(samples, labels, learning_rate, epochs):\n",
        "\n",
        "  # TODO n_samples: number of training samples / n_features: number of features\n",
        "  n_samples, n_features =\n",
        "\n",
        "  # TODO: initialize extended weight vector (threshold included) randomly\n",
        "  weights =\n",
        "\n",
        "  # TODO: extend samples by '-1' column (threshold trick)\n",
        "  samples =\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "\n",
        "    # TODO: generate randomly permuted index array\n",
        "    indexes =\n",
        "\n",
        "    # iterate through all indexes in the index array\n",
        "    for index in indexes:\n",
        "\n",
        "      # TODO: select training sample and corresponding class label according to generated random permutation\n",
        "      sample =\n",
        "      label =\n",
        "\n",
        "      # TODO: classify selected training sample with current weights\n",
        "      classification =\n",
        "\n",
        "      # TODO: adapt weights, i.e. apply perceptron learning rule\n",
        "      weights =\n",
        "\n",
        "      # yield threshold, weight vector, and current sample\n",
        "      yield {'threshold': weights[0],\n",
        "              'weights': weights[1:],\n",
        "              'samples': [sample]}\n",
        "\n",
        "samples, labels = utils.load_data('data/data_2.npz')\n",
        "animation = utils.Animation(samples, labels)\n",
        "generator = learn_perceptron(samples, labels,\n",
        "                             learning_rate=0.01,\n",
        "                             epochs=1)\n",
        "animation.animate(generator,\n",
        "                  max_frames=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7U8ZhF3L8zw"
      },
      "source": [
        "## Exercise 2.2: The DoubleMinOver Learning Rule\n",
        "From the lecture, you know that the DoubleMinOver (DMO) learning rule can be used for *maximum margin* classification. The DMO algorithm is summarized below. Note in particular that the `weights` $\\boldsymbol{w} \\in \\mathbb{R}^{1 \\times N}$ (i.e. the threshold trick is *not* applied) and that an explicit `threshold` $\\theta \\in \\mathbb{R}$ is used, which is computed after learning has been completed.\n",
        "\n",
        "\n",
        "\n",
        "for $t=1$ to $t_{\\max}$\n",
        "> $\\boldsymbol{x}^{+}_{\\min} = \\underset{\\boldsymbol{x}_i \\in X^{+}}{\\operatorname{argmin}} s_{i} \\boldsymbol{x}_{i} \\boldsymbol{w}^{T} \\left( X^{+} = \\left\\{ \\boldsymbol{x}_{i} \\mid s_{i} = 1 \\right\\} \\right)$  \n",
        "> $\\boldsymbol{x}^{-}_{\\min} = \\underset{\\boldsymbol{x}_i \\in X^{-}}{\\operatorname{argmin}} s_{i} \\boldsymbol{x}_{i} \\boldsymbol{w}^{T} \\left( X^{-} = \\left\\{ \\boldsymbol{x}_{i} \\mid s_{i} = -1 \\right\\} \\right)$  \n",
        "> $\\boldsymbol{w} = \\boldsymbol{w} + \\left( \\boldsymbol{x}^{+}_{\\min} - \\boldsymbol{x}^{-}_{\\min} \\right)$\n",
        "\n",
        "$\\theta = \\frac{\\left(\\boldsymbol{x}^{+}_{\\min} + \\boldsymbol{x}^{-}_{\\min}\\right) \\boldsymbol{w}^{T}}{2}$\n",
        "\n",
        "**Tasks**:\n",
        "- Implement the DoubleMinOver algorithm in Python.\n",
        "- Test your implementation on the training data set `data_2.npz`.\n",
        "\n",
        "**Questions**:\n",
        "- Compare your DMO learning results with your perceptron learning results. Run both learning algorithms several times. What differences do you observe in the behaviour of the two algorithms?\n",
        "\n",
        "**Answers**:\n",
        "-\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOM6HMVhL8zy"
      },
      "source": [
        "# TODO: implement the double-min-over learning rule\n",
        "def learn_dmo(samples, labels, epochs):\n",
        "\n",
        "  # TODO n_features: number of features\n",
        "  n_features =\n",
        "\n",
        "  # TODO: initialize weights (threshold not! included) randomly\n",
        "  weights =\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "\n",
        "    # TODO: extract training samples of class +1\n",
        "    samples_pos =\n",
        "\n",
        "    # TODO: get sample_pos_min\n",
        "    sample_pos_min =\n",
        "\n",
        "    # TODO: extract training samples of class -1\n",
        "    samples_neg =\n",
        "\n",
        "    # TODO: get sample_neg_min\n",
        "    sample_neg_min =\n",
        "\n",
        "    # TODO: adapt weight vector, i.e. apply DMO learning rule\n",
        "    weights =\n",
        "\n",
        "    # TODO: calculate threshold\n",
        "    threshold =\n",
        "\n",
        "    # yield threshold, weight vector, and current samples\n",
        "    yield {'threshold': threshold,\n",
        "            'weights': weights,\n",
        "            'samples': [sample_pos_min, sample_neg_min]}\n",
        "\n",
        "samples, labels = utils.load_data('data/data_2.npz')\n",
        "animation = utils.Animation(samples, labels)\n",
        "generator = learn_dmo(samples, labels,\n",
        "                    epochs=50)\n",
        "animation.animate(generator,\n",
        "                  max_frames=50)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}